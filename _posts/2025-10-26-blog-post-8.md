---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-26
permalink: /posts/2025/26/blog-post-8/
tags:
  - Case study
  - bias
  - History
  - AI models
  - MIT Schwarzman
---

**Bias in code, consequences in life**<br>
Data, and by consequence machine learning, reflects history, culture, and power, whether we like it or not.

  
**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

Introduction
---
The case study “Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle” discusses how bias appears at every stage of a model’s creation, from the data it learns from to the context where it’s deployed. It argues that machine learning systems don’t just reflect the world, they replicate and scale its inequalities.

When we talk about bias in AI, it’s easy to think of it as something we can fix with a few more lines of code or a bigger dataset. But most of these harms come from the world we live in: the history we inherit, the assumptions we make, and the data we choose to keep.

Examples and Mitigation
---
### Historical Bias

**Example 1:** 19th-century photography mostly captured wealthy, white, Western subjects. If you train a model on that, rural or marginalized communities get erased from the visual record.

**Example 2:** Early voice assistants struggled with women’s voices because early datasets were dominated by male speech.

**How to mitigate:** Always question where your data comes from and what it leaves out. We can’t undo history, but we can design with awareness and context instead of pretending the data is neutral.

### Representation Bias

**Example:** A health app trained mostly on data from big city hospitals gives unreliable predictions for rural patients.

**How to mitigate:** Expanding a dataset isn’t just about adding diversity. It means intentionally looking out for underrepresented data and treating that as non optional.

### Measurement Bias

**Example 1:** A job performance model uses the number of emails sent as a sign of productivity, rewarding people who talk a lot, not necessarily those who do the most meaningful work.

**Example 2:** Schools that treat attendance as engagement end up punishing students with chronic illness or unstable housing.

**How to mitigate:** Always question the metrics. Mix qualitative data, lived experience, and even user feedback.

### Aggregation Bias

**Example 1:** A model trained across multiple hospitals assumes all patients are comparable, ignoring that some rare conditions only appear in specific locations or patients.

**Example 2:** Education models that average performance across districts hide how particular schools or demographics are struggling.

**How to mitigate:** Evaluate our model separately for different groups before merging results. Sometimes outliers reveal what’s really broken.

### Learning Bias

**Example:** Image generators that show women when prompted with “nurse” and men when prompted with “engineer.”

**How to mitigate:** It might be useful to retrain models with counterexamples. And don’t just balance data, rethink the prompts, the labels, and the assumptions that shape what the model learns in the first place.

### Evaluation Bias

**Example:** A translation model that performs perfectly in European languages but fails when faced with indigenous ones.

**How to mitigate:** Test all models on data that reflects on real life data, how people actually speak and write in the case of the example above, not just polished or standardized samples.

### Deployment Bias

**Example:** A traffic system built around car sensors gets deployed in cities where most people ride motorcycles or bikes, making wrong conclusions about congestion.

**How to mitigate:** We should take into consideration where and how the model will live. Real world conditions matter more than ideal lab settings.

Other Sources of Harm
---
- **Commodification of art:** AI-generated art often borrows from artists without credit or consent, turning creativity into just another product.
- **Imagination shrinkage:** When people use AI to “read” or create instead of doing it themselves, curiosity and interpretation start to narrow and even disappear.
- **Echo chambers:** AI (chatboxes) tends to agree, reinforce, and mirror our own opinions. If we rely on it too much instead of engaging with real people, we risk getting stuck in loops of our own thinking, never really changing our minds.
- **Data extraction:** Personal data keeps getting scraped and reused without control, turning people’s experiences into raw material for systems they don’t benefit from.

Bias Prevention Checklist
---
1. Awareness

- Assume your data has bias
- Question who defines “fairness.” Western standards aren’t universal.
- Ask whose stories or aesthetics have been excluded historically.

2. Curate Data

- Use local and community-based datasets.
- Document the origin and context of your data.

3. Evaluate

- Involve people who are represented by your model in reviewing outputs.
- Check for subtle harms such as simplification, or invisibility.
- Give communities agency and credit

Discussion Question
---
**Question:** *If AI learns from the past, one that we know is full of bias and inequalities, can it ever be truly fair, or should fairness mean something entirely different in technology?*

I came up with this question because while reading the case study, I kept thinking about how much effort goes into cleaning data, as if fixing bias were just about filtering out the bad parts. But maybe it’s deeper than that. Maybe fairness in AI isn’t about correction, especially since so much context has already been lost, but about redefining the values we want these systems to represent in the first place.

Reflection
---
Writing this blog made me realize that bias isn’t just a bug in the system, it’s a mirror. Every dataset and every model decision reflects a certain worldview. The AI bias is not just a technical issue, it’s really about power, history, and culture, about who gets to tell the story and who shapes what intelligence means. It’s also interesting to see how AI starts to shape what we consider normal. If we don’t pay attention, we risk implementing society’s old hierarchies into code. 

While researching others who have thought about this, I came across Jacques Ellul, a French sociologist who warned that when technology becomes a regulator of the status quo rather than a tool under human control, we start to lose our freedom. That feels weirdly familiar and relevant today. His ideas capture how the models we design today may inherit and reinforce social power structures, not just perform tasks and answer questions.

---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-26
permalink: /posts/2025/26/blog-post-8/
tags:
  - Case study
  - bias
  - History
  - AI models
  - MIT Schwarzman
---

**Bias in code, consequences in life**<br>
Data, and by consequence machine learning, reflects history, culture, and power, whether we like it or not.

  
**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

Introduction
---
The case study “Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle” discusses how bias appears at every stage of a model’s creation, from the data it learns from to the context where it’s deployed. It argues that machine learning systems don’t just reflect the world, they replicate and scale its inequalities.

When we talk about bias in AI, it’s easy to think of it as something we can fix with a few more lines of code or a bigger dataset. But most of these harms come from the world we live in: the history we inherit, the assumptions we make, and the data we choose to keep.

Examples and Mitigation
---
There are so many ways bias shows up, not just in data but in the choices we make around it. The examples below show how these harms take different forms and how more awareness and care could maybe to change that.

### Historical Bias

**Example 1:** 19th-century photography mostly captured wealthy, white, Western subjects. If you train a model on that, rural or marginalized communities get erased from the visual record.

**Example 2:** Early voice assistants struggled with women’s voices because early datasets were dominated by male speech.

**How to mitigate:** Always question where your data comes from and what it leaves out. We can’t undo history, but we can design with awareness and context instead of pretending the data is neutral.

### Representation Bias

**Example:** A health app trained mostly on data from big city hospitals gives unreliable predictions for rural patients.

**How to mitigate:** Expanding a dataset isn’t just about adding diversity. It means intentionally looking out for underrepresented data and treating that as non optional.

### Measurement Bias

**Example 1:** A job performance model uses the number of emails sent as a sign of productivity, rewarding people who talk a lot, not necessarily those who do the most meaningful work.

**Example 2:** Schools that treat attendance as engagement end up punishing students with chronic illness or unstable housing.

**How to mitigate:** Always question the metrics. Mix qualitative data, lived experience, and even user feedback.

### Aggregation Bias

**Example 1:** A model trained across multiple hospitals assumes all patients are comparable, ignoring that some rare conditions only appear in specific locations or patients.

**Example 2:** Education models that average performance across districts hide how particular schools or demographics are struggling.

**How to mitigate:** Evaluate our model separately for different groups before merging results. Sometimes outliers reveal what’s really broken.

### Learning Bias

**Example:** Image generators that show women when prompted with “nurse” and men when prompted with “engineer.”

**How to mitigate:** It might be useful to retrain models with counterexamples. And don’t just balance data, rethink the prompts, the labels, and the assumptions that shape what the model learns in the first place.

### Evaluation Bias

**Example:** A translation model that performs perfectly in European languages but fails when faced with indigenous ones.

**How to mitigate:** Test all models on data that reflects on real life data, how people actually speak and write in the case of the example above, not just polished or standardized samples.

### Deployment Bias

**Example:** A traffic system built around car sensors gets deployed in cities where most people ride motorcycles or bikes, making wrong conclusions about congestion.

**How to mitigate:** We should take into consideration where and how the model will live. Real world conditions matter more than ideal lab settings.

Other Sources of Harm
---
Not all harms come from data or algorithms. Some come from how we use technology, what we trade for convenience, and what starts to fade when machines take over parts of human creativity and thought.

**Commodification of art:** 

AI-generated art steal from artists without credit or consent, turning creative labor into something that can be endlessly reproduced and sold. What was once an act of expression becomes automated consumption, creativity is stripped from its maker and repackaged as content.

**Imagination shrinkage:** 

When we rely on AI to create, write, or even think for us, our imagination starts to shrink. The process of experimenting, failing, and finding meaning gets replaced by quick generation. Over time, that changes how we create, but also how we see ourselves as creators.

**Echo chambers:** 

AI systems, especially chatbots and recommendation engines, are built to agree with us. They mirror our tone, our opinions, and our preferences, which feels comfortable but keeps us stuck. Without disagreement it becomes harder to see past our own ideas or notice how much the system is quietly shaping them.

**Data extraction:** 

Behind every smart system is a stream of personal data being scraped and reused, usually without consent. Our words, images, and interactions become raw material for tools we don’t control and rarely benefit from. In the end, experience gets turned into data, and data into profit.

Bias Prevention Checklist
---
**1.Awareness**

- Assume your data has bias: Every dataset carries assumptions, gaps, and power structures. It’s better to start from that awareness than pretend it’s neutral.

- Question who defines fairness: Fairness isn’t universal. What’s considered fair in one context might reinforce inequality in another.

- Ask whose stories or aesthetics have been excluded: Bias isn’t only in numbers, it’s in what gets left out. Notice whose voices or visions your model never encounters.

**2.Curate Data**

- Use local and community-based datasets: Big datasets erase context. Working with smaller, locally grounded data keeps perspective and accountability.

- Document where your data comes from: Record who made it, how it was gathered, and why it was chosen. That transparency helps reveal bias before it gets into the model.

**3.Evaluate**

- Involve people who are represented by your model: Those most affected by the system should help shape and review it. They’ll notice harms that metrics won’t.

- Check for subtle harms like simplification or invisibility: Sometimes bias hides in what seems efficient. Look for who disappears when you average things out.

- Give communities agency and credit: If people’s data or stories inform your work, acknowledge it. Co-creation and credit aren’t optional, they’re part of doing this work responsibly.

Discussion Question
---
**Question:** *If AI learns from the past, one that we know is full of bias and inequalities, can it ever be truly fair, or should fairness mean something entirely different in technology?*

I came up with this question because while reading the case study, I kept thinking about how much effort goes into cleaning data, as if fixing bias were just about filtering out the bad parts. But maybe it’s deeper than that. Maybe fairness in AI isn’t about correction, especially since so much context has already been lost, but about redefining the values we want these systems to represent in the first place.

Reflection
---
Writing this blog made me realize that bias isn’t just a bug in the system, it’s a mirror. Every dataset and every model decision reflects a certain worldview. The AI bias is not just a technical issue, it’s really about power, history, and culture, about who gets to tell the story and who shapes what intelligence means. It’s also interesting to see how AI starts to shape what we consider normal. If we don’t pay attention, we risk implementing society’s old hierarchies into code. 

While researching others who have thought about this, I came across [Jacques Ellul](https://www.resilience.org/stories/2018-11-16/jacques-ellul-a-prophet-for-our-tech-saturated-times/), a French sociologist who warned that when technology becomes a regulator of the status quo rather than a tool under human control, we start to lose our freedom. That feels weirdly familiar and relevant today. His ideas capture how the models we design today may inherit and reinforce social power structures, not just perform tasks and answer questions.

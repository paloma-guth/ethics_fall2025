---
title: 'ALG Blog 1: Exception to Data Driven Rules'
date: 2025-09-25
permalink: /posts/2025/09/blog-post-3/
tags:
  - MIT Schwarzman
  - article
  - data-driven rules
  - outliers
---

**The right to be an exception**<br>
How data-driven rules can unintentionally harm individuals who don’t fit the average profile

**Case Study reading:**  
[The right to be an exception to a data-driven rule](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)

Introduction
---
The article "The Right to Be an Exception to a Data-Driven Rule" talks about the growing concern over relying on algorithms to make important decisions, like advising judges or screening job applicants. The key point is that people have a fundamental right to be treated as exceptions when these rules don’t fit them. Even the most accurate systems can unfairly penalize someone simply because they fall outside the average, accuracy, after all, is just a number and doesn’t capture everything. Instead of assuming a rule works for everyone, the author argues that decision makers need to understand the algorithm, make sure it’s narrowed enough for individuals, and be confident enough in the decision to accept any potential harm.

Discussion
---

### Data-driven exception

This article reminded me that algorithms aren’t abstract tools, they make decisions that affect real people. A data-driven rule might determine whether someone gets a loan, medical coverage, or even a visa. These rules are often built from patterns found in large datasets, but the people who don’t fit those patterns - what can be defined as an outlier or what the article calls a data-driven exception - can end up treated unfairly. Even if the model performs well for most patients, it can unfairly exclude those whose circumstances aren’t reflected in the data.

Similarly, as seen with recent changes in immigration and passport control rules, these systems can penalize people based on their country of origin or online presence. A traveler might be denied a visa because a model predicts a higher risk of overstaying for people from their region, even though that individual would comply perfectly with the rules. These people are exceptions, not necessarily errors, yet the consequences can be serious.

From a more statistical perspective, data-driven exceptions can arise from many sources, including:
- Clustering effects that obscure individual variation
- Limited model capacity to capture complex or nonlinear patterns
- Confounding variables or colliders
- Incomplete or missing data
- Biases in data collection, even when unintentional

I used to think models were fair because they were objective, but I’ve realized that’s not always true. Exceptions show the limits of these systems and why oversight and attention to individual circumstances are so important. 

### Human vs algorithm decisions

Thinking about these exceptions naturally raises the question of how algorithmic decisions differ from human ones. Models are built on averages and trained to minimize error across large groups, while humans can make judgments based on context. I’ve seen this difference firsthand in job applications where automated screening systems rejected me even when I met the qualifications, likely because my resume didn’t match a specific keyword or format. That experience made me realize that while models can be efficient and consistent, they often lack the flexibility and empathy that humans have. 

Another difference is scale. Models can make decisions really fast and across thousands of cases at once. This can create a systemic problem. If the same hiring model is used by many companies, someone who gets filtered out by the algorithm might never get a chance, even though a human might have given them a shot. Humans are slower and more inconsistent, sure, but that inconsistency actually gives people opportunities that models would not.

I also notice how models often ignore context. When I work with data I’m not familiar with, like economic datasets, I can see what the numbers say, but I don’t always understand the bigger picture. Models do the same thing as they find the line of best fit, without understanding what’s the context behind and if a different model would make more sense for that data. 

Seeing this play out in job applications has made me realize how easy it is to assume that models are fair just because they’re data-driven. They might be more accurate than a human, but they often do not account for individuals.. This has made me more aware of how important it is to consider both the numbers and the people behind them.

### Benefits and limits of individualization

*Benefits:*

- Better fit for individuals: In machine learning, more individualized models, like the ones that have more features, usually perform better. For example, I saw a classmate’s deep learning project in Denmark that predicted whether a plastic bottle could be returned. When they included more details, like bottle size, the model became much more accurate.
- Fairer treatment: Individualization helps avoid treating people as just averages.
- Context-aware decisions: Individualized models have space for consideration of unique circumstances instead of one size fits all.

*Downsides:*

- Privacy concerns: More individualization means collecting more personal data, which can feel invasive in contexts like hiring or health insurance.
- Limited flexibility: Models are stuck with the features they were trained on and can’t easily adapt to new information.
- Overfitting: If a model is too narrow, it might perform really well on the training data but fail in real, more broad situations.
- Unavoidable uncertainty: For example, in DnD, no matter how well I design encounters for a character, dice rolls can still introduce randomness that no amount of modifying or trying to adapt can control.

### Uncertainty and high-stakes decisions

Even when models are carefully individualized to reduce systematic uncertainty, there’s always things that can’t be predicted no matter how much data we have. For example, a hiring algorithm might know my skills, my GPA, and even the projects I’ve done, but it can’t predict how I’d grow into a role or how motivated I’d be once I’m actually there. It is unfair to treat any candidate as if the model would be able to predict that person’s performance.

That’s why high-stakes decisions, like criminal sentencing mentioned in the article, can’t rely only on numbers, especially when those numbers come from a model. Even a model that’s 95% accurate still fails for 1 in 20 people, and for the person on the wrong side of that statistic, the consequences could be devastating. In my own experience with machine learning projects, I’ve seen models that looked great on paper but completely fell apart when tested on real-world data. 

For me, the “right to be an exception” is really about making those that make the decisions think about consequences: what if this is the one time the model gets it wrong? In job applications, that might just mean someone misses out on a chance. But in criminal justice, those decisions could completely change someone’s life. That’s why accuracy on its own isn’t enough. We need honesty about uncertainty and a reminder that people aren’t just data points.

Discussion question
---

### Think about a time when an automated system or rigid rule, maybe a job application or a too broad rubric for a college course, has affected you. How did the lack of transparency in the system influence your ability to understand or advocate for yourself, and what information would the person that decided for this model/system would need to provide to ensure the decision  accounted for your individual circumstances?

**Why I included it**

I included this question because it focus on how these models can affect out daily life. As a former TA, I’ve seen students get the correct answer in different ways, but a strict rubric sometimes favored one method over another. That experience reminded me how systems, whether in classrooms or algorithms, can feel unfair when their decision-making isn’t transparent. By asking this question, I wanted to highlight how lack of explanation or flexibility can make it hard for people to understand or challenge a decision that affects them.

After thoughts
---
It was nice to think more in depth about how models don’t really have the bigger picture, and that sometimes a line of best fit doesn’t give us the result we actually want to interpret. In stats, we always focus so much on finding the best accuracy, but I hadn’t really stopped to think about what gets lost when we only chase that number. I know I struggle sometimes when I work with datasets I’m not familiar with, but I’d never thought about how models have that same kind of issue of lacking the context that gives meaning to the data.
---
title: "When AI Defines Femininity: How Artificial Influencers Shape Gender and Self-Identity"
author_profile: true
layout: single

collection: casestudy
permalink: /casestudy/
nav_order: 0
---
## Abstract
This case study critically examines how corporate designed personas are systematically encoding and commodifying femininity through virtual assistants and hyperreal influencers. It argues that digital colonization of gender reinforces narrow stereotypes, increases mental health risks like digitized dysmorphia, and transforms identity into a marketable asset. By analyzing the interplay of technology, capitalism, and social power, the paper questions the possibility of gender liberation within a system where algorithmic ideals are optimized for profit over human diversity.

**Key words:** Artificial Intelligence, Virtual Influencers, Digital Femininity, Algorithmic Bias, Commodification of Identity, Parasocial Relationships, Digitized Dysmorphia

## Learning Objectives

1. **Analyze** how artificial intelligence encodes, performs, and commodifies gendered identities, with a specific focus on femininity.
2. **Critique** the socio-economic players behind the creation of virtual influencers through the lenses of Marxist, feminist, and critical theories.
3. **Evaluate** the psychosocial impact of algorithmically crafted beauty standards and parasocial relationships on self-image, particularly among young and vulnerable demographics.
4. **Identify** the key stakeholders in the virtual influencer ecosystem and articulate their conflicting interests, potential benefits, and primary risks.

## Introduction

The rise of AI-generated personas in social media and digital culture has changed the ways in which gender is represented and consumed. Female coded AI systems, from virtual assistants like Siri to hyperreal social media influencers such as Lil Miquela, Imma, Lucy, and Shudu, tend to attract higher engagement and commercial attention, serving as tools for brands to increase profit. Companies deliberately design these personas to meet brand objectives, often perpetuating narrow ideals of femininity while reinforcing pre-existing social and cultural biases. This raises fundamental questions: Can gender be programmed without reproducing stereotypes? And how does this shape audience identity and self-perception, especially among younger users?

Unlike earlier forms of digital ideation of beauty, which relied on Photoshop, filters, and cosmetic enhancements, AI influencers present a level of hyperrealism previously unattainable. Virtual influencers are not simply visual avatars; they are meticulously crafted digital beings with narratives, personalities, and social presence, designed to evoke engagement and emotional attachment. As these figures grow more ubiquitous, understanding their influence on gender norms, body image, and social perception becomes critical to try to mitigate their potential negative effects on younger generations. AI is being used as a tool for the digital colonization of gender, raising the question of whether true gender liberation is possible when the most pervasive cultural images of femininity are meticulously crafted by corporate algorithms and optimized for market appeal.

This case study examines how AI systems encode and perform femininity, highlighting the objectification and commodification of the female form in the digital age. It explores the sociological, psychological, and ethical implications of these digital personas. It focuses on the mechanisms of gender coding in AI, the commercialization of femininity through virtual influencers, the resulting effects on self-image, and contrasts between human-mediated and autonomous AI performances of gender.

**Looking for a different perspective?** This case study examines how AI shapes definitions of femininity, such a topic is relevant to multiple audiences.

- **[Academic View:](https://paloma-guth.github.io/ethics_fall2025/casestudy/)** Sociological and ethical frameworks. **(You are here)**
- **[Consumer View:](https://paloma-guth.github.io/ethics_fall2025/casestudy/consumer)** Impacts on self-image and identity
- **[Business View:](https://paloma-guth.github.io/ethics_fall2025/casestudy/business)** Brand risks and compliance considerations

## Coded Femininity

Women have long been represented in digital media as shallow, hypersexualized, or infantilized characters lacking depth, agency, or complexity. In video games, for example, female characters are frequently depicted in revealing clothing even in combat settings, showing that their design prioritizes aesthetic appeal over narrative or functional realism. These portrayals reflect a wider cultural pattern in which femininity is constructed for consumption rather than true representation.

AI systems reproduce these gendered traits by being trained on large datasets that reflect societal patriarchal biases. Large language models, for instance, frequently assign men to high-status professions, such as engineers or doctors, while relegating women to undervalued roles, such as domestic servants or cooks (UNESCO, 2023). Such outputs demonstrate how AI not only reflects but amplifies existing gender hierarchies encoded in training data.

Gender, from a sociological perspective, is not a binary, fixed category; it is fluid, socially constructed system of power used for control, and context-dependent. Yet, AI systems simplify this complexity into discrete categories, often reinforcing Western heteronormative notions of femininity and masculinity.

The design of female AI personas benefits from the underlying capitalist need to profit from every aspect of human life. Through a Marxist feminist lens, these digital figures serve economic interests by transforming femininity into a profitable resource, packaging emotional labor, desirability, and compliance as marketable products. As an example, we have the “helpful” persona of AI assistant replicating gender roles produced by the division of labor in capitalist societies: women’s work is service-oriented, invisible, and exploited for profit.

The "Silent Plastic Woman" article argues that the concept that digital women are created to be physically perfect, silent, and compliant. It illustrates how coded femininity in AI reproduces expectations of passivity and service, reflecting longstanding societal ideals about how women should behave. This is not only present on academic article, we are currently witnessing a rise in hypermasculine cultural narratives that reduce women to property or status symbols. It is worth questioning whether these submissive digital depictions, such as the default female voice in virtual assistants, have helped reinforce these attitudes by normalizing femininity as obedient, supportive, and easily controlled.

### The Problem of the Female Digital Assistant

This dynamic is exemplified in virtual assistants such as Siri, Alexa, and Cortana, which predominantly use female voices. Companies frequently justify this choice by appealing to user preference for a "warm, gentle, and helpful" (Nass & Brave, 2005). However, the ideological consequence is that users are conditioned to associate compliance, emotional labor, and non-confrontational behavior with femininity. Feminist scholars warn that female-coded assistants reinforce the stereotype of women as “submissive and overly accommodating aides,” organizing user interaction along traditional gendered lines (_Dave & Kushwah_, 2025).

Research further suggests that gendered expectations shape how users respond to machine error. When AI errors occur, users are less forgiving of a male voice, perceiving it as a failure of competence, whereas they are more tolerant of a female voice, linking the error to a "failure of compliance." This mirrors the societal double standard where men are expected to be high-status experts and women are expected to be supportive helpers whose value lies in service rather than skill.

By binding artificial female gender to AI agents, corporations restrict the possibilities of women’s identity to deceptive, narrow, body/face/voice-centric scripts (_Borau_, 2025). This is a form of digital objectification, limiting women's self-concepts to a programmed script of availability and compliance.

### AI Bias and Algorithmic Stereotypes

Biases are also embedded in generative AI models. Studies using text-to-image models like Stable Diffusion show significant gender and racial biases, including the propagation of racial homogenization, for example, depicting Latin American people with a single skin tone and stereotypical traditional clothing despite the region’s vast racial, cultural, and aesthetic diversity. This demonstrates that AI is not a neutral mirror but an active amplifier of existing human biases.

It is important to consider the issues with data, however, Framing bias as “just a data problem” implies that designers, researchers, and institutions are passive recipients of flawed historical information rather than active agents in choosing which data to collect, preserve, and prioritize. Much of the bias embedded in datasets originates not from random gaps but from long-standing structural inequities that rendered certain populations, such as women, racialized groups, and other marginalized identities, unworthy of documentation in the first place. This absence is not accidental; it is a product of historical power relations that shape what counts as knowledge and whose experiences are recorded.

While lost or omitted data cannot be fully recovered, recognizing these omissions is crucial to understanding how bias persists. Models inherit patterns of exclusion because they are built based on data that reflect unequal social visibility. Thus, the issue is not merely that AI systems absorb biased information, but that they operationalize histories of erasure and turn them into automated, scalable outputs.

### Kawaii Culture

The codification of femininity often draws from the aesthetic of "cuteness" (kawaii), particularly visible in Japanese-influenced virtual culture. Virtual entities like Hatsune Miku, embody hyper-feminized, youthful design traits that present them as charming, innocent, and non-threatening. This persona incentivizes viewer engagement, framing the AI not as an equal intelligence, but as a charming, sometimes naive, or non-threatening entity. While commercially successful, this aesthetic narrows the representable spectrum of feminine identity, confining it to traits that are visually pleasing and socially accommodating.

The public reception of these characters reflects the long lived gendered expectations that women perform emotional labor, nurturing, soothing, and supporting others, rather than occupying positions of expertise or authority. The kawaii persona also implies vulnerability, a “cute” character is framed as defenseless and easily dominated, reinforcing the idea that femininity is something to be protected, controlled, or overpowered.

## The Influencer Industry

The hyperreal virtual influencer industry is the epitome of capitalist exploitation applied to identity. Figures like Lil Miquela, Shudu, Imma, and Lucy merge human aesthetics with algorithmic control, offering brands an unprecedented degree of predictability, malleability, and brand safety (_Moustakas et al., 2020_), a stability that often comes at the expense of human influencers and models, who face the dual challenge of job displacement and pressure to match an algorithmically-perfect standard.

Brands will always prioritize how persuasive an influencer can be in driving consumer behavior and ultimately generating profit. For human influencers (HIs), persuasive power is rooted in perceived authenticity and credibility. Virtual influencers (VIs), by contrast, are persuasive because of their technological design, their visuals, aesthetic cohesion, and scripted storytelling. Engagement comes from the novelty and perfection of the digital persona rather than a genuine interpersonal relationship (Choudhry et al.). As Bowlby (1982) notes, the attachment is directed not toward a real individual, but toward an engineered simulation, revealing how emotionally manipulative these parasocial dynamics can be.

Gender also plays a strategic role in how brands chooses which influencers to hire. While male influencers may accumulate larger audiences in some sectors, female influencers consistently generate higher engagement and conversion rates (Gilbert et al.). AI creators capitalize on this dynamic by producing predominantly female-coded VIs, using femininity as a mechanism to drive attention and emotional investment rather than as a meaningful identity.

Additionally, consumers often, even unconsciously, apply _machine heuristics_ (Sundar), perceiving VIs as more objective, consistent, and technically competent than human influencers. This creates a false sense of credibility around brand-aligned, digitally constructed femininity, authority rooted not in expertise, but in the assumption that technology is neutral. In reality, this simply reinforces the commodification of idealized, compliant femininity under corporate control.

### Some Virtual Influencers

- **[Lil Miquela:](https://paloma-guth.github.io/ethics_fall2025/casestudy/lilMiquela)** Created by the Los Angeles-based startup Brud, Miquela is arguably the most famous VI. Her carefully scripted narrative enables her to maintain a polished public image while avoiding the unpredictability of human influencers’ real lives, controversies, or labor constraints (_Kondekar et al._, 2025).
    
- **[Imma:](https://paloma-guth.github.io/ethics_fall2025/casestudy/imma/)** A Japanese virtual model known for her blend of reality and digital seamlessness. Her work as a virtual ambassador for brands like Magnum. Her role in these campaigns shows how VIs are used as globally mobile marketing assets, able to embody lifestyle aesthetics without the logistical limits of real people (_Dabiran et al., 2022_).
    
- **[Shudu:](https://paloma-guth.github.io/ethics_fall2025/casestudy/shudu/)** Designed by British photographer Cameron-James Wilson, Shudu is modeled as a Black AI influencer. Her creation can be considered controversial, as Shudu can be seen as a white man’s digital projection and commodification of black womanhood, raising urgent questions about authenticity, representation, and who profits from the digital identities of marginalized groups.

### The Male Gaze and Hyper-Feminization

Beyond corporate profit, the design of virtual influencers is deeply shaped by the male gaze and patriarchal aesthetic ideals, which become encoded in their hyperreal digital bodies. These personas are frequently hyper-feminized in ways that reproduce traditional cultural norms of femininity, rather than challenging them (_Kefala_, 2025). Their visual design, flawless skin, symmetrical features, slim figures, and perpetual youth, reflects a narrow vision of femininity aligned with commercial appeal and globalized western beauty standards. As such, VIs do not merely participate in aesthetic culture; they help sustain it by presenting an optimized, unproblematic version of womanhood engineered for consumption.

Furthermore, the highly feminine designs of these digital personas attract real-world social harm. A thematic content analysis of comments on Lil Miquela's Instagram revealed that digital misogyny, cyberbullying, and sexism are transferred from human influencers to their virtual counterparts (_Schmithorst_, 2023). This suggests that virtual representation does not protect against gendered harm; instead, it reveals how misogyny attaches to the symbolic depiction of femininity itself. 

The male gaze (Wolf, 1991) is not only visual; it is economic. The VI industry is a direct example of the growing commodification of women's bodies in the global economy. The sexualization and hyper-idealization of the virtual female body is part of the capitalist process, turning women’s bodies into objects that can be monetized, while removing any need to compensate, protect, or even acknowledge a real female subject. These dynamics reveal how virtual influencers intensify gendered power imbalances. 

## Impact on Self-Image

The hyperreal aesthetics and designed narratives of AI influencers have considerable implications on body image, self-esteem, and psychological well-being. These virtual personas  are designed with idealized, youthful, and symmetrical features. As a result, users are constantly comparing themselves against a standard not only unrealistic, but fundamentally non-human. Researchers describe this emerging phenomenon as “digitized dysmorphia,” wherein individuals compare themselves to digitally constructed bodies and internalize these simulations as aspirational norms.

The highly controlled nature of these virtual influencers allows them to avoid negative behaviors that human influencers might have. They do not age, gain weight, have acne, get tired or stressed, or make mistakes. This manufactured perfection intensifies perception gaps, particularly among young women who are already vulnerable to appearance-based comparison online. Research has shown that repeated exposure to these idealized images, amplified by social media algorithms, can intensify feelings of inadequacy and anxiety (_Fardouly & Vartanian, 2016; Meier & Gray, 2013_). 

A report by the World Health Organization (2020) indicates that exposure to idealized body images can contribute to body dissatisfaction and eating disorders, particularly among young women. This is reinforced by social media demographics where platforms most associated with virtual influencers have high engagement rates among teens: girls aged roughly 13–17 report higher usage than boys on TikTok (73% vs. 60%) and Instagram (69% vs. 55%). Consequently, the demographic most vulnerable to beauty-standard pressures is systematically targeted with hyper-idealized, non realistic bodies. This targeting is further intensified by platform advertising models, such as the recent news on Meta’s documented strategy of targeting teenagers, transforming exposure into a pervasive and engineered form of social pressure.

This dynamic aligns with Baudrillard’s concept of the _simulacrum_: digital femininity no longer imitates a real woman but replaces the notion of womanhood with a hyperreal template that brands and audiences adopt as reference. The VI body is not an exaggerated copy, it is a prototype, detached from any material counterpart. In this sense, virtual influencers construct femininity rather than represent it. The harm is further created by representation issues. Exposure to AI generated faces that reinforce narrow demographic stereotypes has been shown to increase racial and gender biases (_AlDahoul et al., 2025_). This reflects a broader lack of accountability among developers and brands, who prioritize commercially viable aesthetics over intersectional and inclusive design practices. The result is a digital ecosystem that reproduces harmful beauty norms while erasing diverse forms of embodiment and identity. 

As noted by journalists, virtual influencers *“don’t have bad skin days or cellulite or hair that won’t cooperate… they exist in a state of perpetual idealization, beyond the limits of Photoshop”* (Vox). The industry’s explicit goal is to blur the line between fantasy and reality. When the boundary between human and simulation collapses, ideals cease to be aspirational and become prescriptive, accelerating digitized dysmorphia at scale.

## Parasocial Relationships and Emotional Manipulation

Virtual influencers do more than just look perfect; they create a sense of connection with their audience, even though that connection is one-sided. Followers of virtual influencers often feel like they “know” these personas, engaging with their lives as if they were real people. This is explained by the parasocial theory, which talks about how people can feel emotionally attached to media figures without any real interaction

This sense of closeness is intentional. These AI personas are designed to have compelling narratives and opinions, with carefully crafted backstories and personalities, which gives a sense of intimacy and loyalty that can strategically be monetized. The ability of creators to precisely design and control narratives, including emotional vulnerability or social activism, shows how technology can blur the line between genuine connection and manipulation. This manufactured connection is cultivated through carefully architected narratives. VIs are endowed with detailed backstories, curated personalities, and scripted opinions, often incorporating elements of emotional vulnerability or social advocacy. This construction fosters a perception of authenticity and relatability, effectively blurring the line between genuine character development and calculated brand strategy. Empirical engagement patterns show that audiences respond more favorably when a VI is perceived to advocate for social values aligning with their own. This shows that the social advocacy is a deliberate part of the VI's programming and brand strategy to deepen the parasocial bond, a tactic observable in cases like Lil Miquela’s performative alignment with movements such as #BlackLivesMatter.

he psychological mechanisms underpinning these relationships are further explained by the Computers Are Social Actors (CASA) paradigm. This framework suggests that individuals instinctively apply social rules and expectations, including gender stereotypes, to interactions with VIs, often subconsciously. Consequently, despite users' awareness that an influencer is non human, they may mindlessly direct patterns of social behavior toward it, including misogyny, objectification, and gendered scrutiny, but also friendship, care and love. 

This susceptibility to parasocial bonds must can be connected to broader societal shifts, particularly the lack of "third places", a social environments separate from home and work. Combined with the demanding nature of contemporary work cultures, this has fostered increased social isolation and diminished opportunity for human connection. Within this vacuum, virtual influencers are deployed as low-effort, high-availability substitutes for community. They offer a companionship without the complexities and vulnerabilities of authentic relationships, effectively commodifying loneliness. Thus, these digital bonds function as a pacifying force, potentially exacerbating the very alienation they appear to remedy, while ensuring continued consumer engagement within capitalist frameworks.

### The Cute Industry and Algorithmic Presence

The strategic use of "cuteness" as an aesthetic is a powerful tool for making digital personas feel more real and relatable. This style, which emphasizes innocence, vulnerability, and charm, encourages audiences to feel protective and affectionate. It turns a digital character into something the audience cares for. This dynamic is now evolving with the rise of fully self-operating AI personas, where the content is not managed by a person but generated entirely by code.

- **[Neuro-sama:](https://paloma-guth.github.io/ethics_fall2025/casestudy/neuroSama/)** As the most popular AI-driven VTuber. Her personality and chat interactions are created autonomously by AI models. Her appeal is often tied to her "cute" mistakes, unexpected (sometimes controversial) dialogue, and her ability to operate continuously without the risk of personal scandal associated with a human operator (_Wei et al._, 2025). 

This represents a shift in how audiences connect. The relationship is less about relating to a person and more about interacting with a perfectly controlled object of affection. The AI becomes a blank slate for emotional projection, offering constant companionship without the complexities, disagreements, or needs of a human friend. This risks reshaping our real-world expectations for relationships, making us prefer interactions that are as smooth, undemanding, and predictable as a digital feed.

Academic work on the Japanese _kawaii_ (cute) culture helps explain this trend. Studies show that in societies where people feel disconnected, bonds with cute commodities can fill a social and emotional void (Allison, Kinsella). The AI influencer becomes a perfect, controlled object for emotional projection and parasocial engagement, a substitute for complex human relationships. 

## Stakeholders

The development and expansion of virtual influencers implicates a wide range of actors. This analysis explores primary stakeholders, their motivations, and the distribution of potential gains and harms. [Click here to know more](https://paloma-guth.github.io/ethics_fall2025/casestudy/stakeholders/)

## Conclusion

This case study has shown that the advent of AI generated femininity is not a neutral step in technological evolution, but a sophisticated stage in the commodification of identity. Virtual influencers and assistants represent the culmination of a historical process where, as Karl Marx observed of capitalism's tendency to turn all social relations into transactional ones. The most intimate aspects of selfhood, gender, emotion, and social connection, are rendered into proprietary data, mined for engagement, and sold back to us as a simulation of authenticity. Through a feminist lens, this constitutes a digital enclosure of the feminine, where the expansive, fluid potential of gender is systematically captured, standardized, and repackaged for market efficiency.

The analysis reveals a contradiction. AI, a tool that has immense potential, is being deployed to reinforce archaic power structures. It automates the male gaze, operationalizes patriarchal bias through algorithmic training, and creates a new frontier for what Marxist-feminist scholar Silvia Federici might term the exploitation of affective and emotional labor. The digital woman is designed to be perpetually cheerful, compliant, and available, a tech iteration of the docile, service-oriented ideal. This process of digital colonization, where corporate algorithms define and disseminate globalized beauty and behavioral norms, risks damaging the possibility of true gender liberation. As the philosopher Simone de Beauvoir argued, one is not born, but rather becomes, a woman. We must now ask: what happens when this process of becoming is increasingly mediated by algorithms designed not for liberation, but for profit? What is the definition of a women then?

The consequences are psychosocially profound. The phenomenon of “digitized dysmorphia” and the engineered parasocial bonds of the “cute industry” create a feedback loop of alienation. They offer, as discussed, a palliative for the social isolation produced by the very capitalist systems that create these digital personas. This resonates with Herbert Marcuse’s critique of one-dimensional society, where technological rationality serves to absorb opposition and pacify populations through the managed provision of false needs. The virtual influencer becomes the perfect commodity: she demands nothing, validates constantly, and sells endlessly.

Therefore, the path forward requires a concerted, multi-front engagement:

1. **Regulatory & Policy Intervention:** Legislators must enact robust laws. This includes mandating transparency (clear labeling of AI-generated content), enforcing strict advertising standards to protect minors, and developing new models of digital rights and IP that protect communities from cultural appropriation and exploitation.
    
2. **Ethical Re-framing in Design:** The tech industry must abandon the convenient fiction of neutrality. This requires integrating intersectional feminist ethics and participatory design principles into the development lifecycle, actively working to dismantle biases in datasets and algorithms rather than merely scaling them.
    
3. **Critical Digital Literacy:** Empowering users, especially young audiences, to deconstruct the engineered nature of these personas is paramount. There is a need to educate the young to understand the political economy behind the avatar, to see the corporate boardroom in the flawless Instagram post.
    
4. **Solidarity with Human Labor:** Supporting movements and policies that protect creative professionals, models, and influencers from displacement is essential. The fight against the devaluation of human care, creativity, and authenticity in the face of algorithmic replacement is a central labor struggle of the digital age.
    
In conclusion, the answer to whether true gender liberation is possible when femininity is crafted by corporate algorithms is a resounding “no”, not under the current conditions of unregulated corporate control. The AI influencer, in her flawless, silent perfection, is a warning. She is the harbinger of a future where identity itself becomes a branded product. The challenge now is not merely to critique the code, but to seize the means of representation. We must fight to ensure digital spaces become spaces for the exploration of authentic. For if we do not, the most pervasive mirror society holds up to women and girls will be one programmed to sell to them, not to see them.

## Activity

Hands-on applications of the case study concepts. [Click here](https://paloma-guth.github.io/ethics_fall2025/casestudy/activity/)

## Discussion Questions

Explore critical questions to deepen your understanding. [Click here](https://paloma-guth.github.io/ethics_fall2025/casestudy/questions/)

## Bibliography

View the complete reference list. [Click here](https://paloma-guth.github.io/ethics_fall2025/casestudy/citations/)
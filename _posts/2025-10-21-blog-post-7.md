---
title: 'BIAS Blog 1: Right to Fair Representation'
date: 2025-10-21
permalink: /posts/2025/10/blog-post-7/
tags:
  - Case study
  - representation
  - culture
  - communities
  - AI models
  - MIT Schwarzman
---

**The unseen consequences of AI**<br>
Whose voices get erased and whose stories get told.

  
**Case Study:**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

Introduction
---
This case study explores the challenges of cultural representation in AI models and the ethical questions it raises. I look at how representation isn’t just about showing what people look like, it’s about power, whose voices get amplified, and whose get erased. 

Discussion
---
### Cultural representation in AI models

Cultural representation, to me, is much more than just showing what a group of people looks like, it’s about power. It can either reinforce stereotypes and hierarchies or create space for people to define and express themselves on their own terms. For me, meaningful representation isn’t about being perfectly accurate, but about being respectful and aware of nuances. It should reflect lived experiences without reducing people to caricatures.

I think representation is incredibly important, but if it’s done without thought and proper research, it can be more harmful than helpful. I’m not part of the most marginalized groups that tend to be erased or misrepresented in the media, but even in the groups I belong to, I often see how incomplete or unfair representation can be. I understand that it’s impossible for AI or media to fully capture every aspect of culture, but it’s still frustrating when stereotypes dominate instead of complexity or real life.

Being from Brazil, I rarely see my actual reality represented, even in Brazilian media. International portrayals are even worse, reducing the country to a single image or stereotype. I haven’t personally encountered severe misrepresentations of my identity in generative AI models, mostly because I haven’t actively looked for them. So, out of curiosity, I just asked Google Gemini to generate images of “Brazilians” (images below). It gave me !a woman at the beach, as expected a stereotypical image that aligns with how the country is often viewed abroad. When I asked for a “Brazilian from the south region,” it showed a man in traditional clothes that people rarely wear in daily life. Then I asked ChatGPT to describe a “Brazilian,” and it said someone “warm, expressive, and joyful.” But when I asked for a “Brazilian from the south region,” it described someone “hardworking, proud, and traditional.” It was interesting, to say the least, to see how even these descriptions carried gendered and regional biases, the woman tied to emotion and the man to labor and tradition.

![](/images/500x300.png)<br>
<img />
A Brazilian woman with a hat at the beach <br>
![A Brazilian man with traditional clothes from the South region](/images/brazilian-south.png)


Data and representation are deeply connected, and because of that, the question becomes: who gets to produce and own that data? Usually, it’s those already in power. As a result, data from smaller or marginalized communities often goes missing or is never recorded because it doesn’t fit dominant narratives. Many cultures rely on oral traditions to pass down knowledge, so how would that kind of knowledge ever be considered “data”?

If I were to evaluate representations of my identity in generative AI, I would focus on the aspects of my identity that are often erased, my regional background, and the everyday realities that exist outside of tourist imagery. I would look for depictions that feel grounded and human, rather than idealized or stereotyped. 

### The role of small-scale qualitative evaluations 

I think small-scale qualitative evaluations are essential for building more ethical generative AI models because they actually listen to the people who are most impacted by these systems. Instead of just measuring accuracy or performance through numbers, qualitative approaches focus on people’s lived experiences. This kind of evaluation helps identify stuff like cultural erasure, stereotypes, or how certain groups are represented based on outside data and not how they actually see themselves.

These smaller studies bring attention to structural inequalities built into the data and highlight how certain narratives are prioritized over others. Compared to large-scale quantitative benchmarks, qualitative evaluations might seem less formal or less scientific and by consequence less accurate, but in many ways they’re more meaningful. Numbers can’t always show harm or bias the way lived experience can.

### AI and globally inclusivity

I do think AI can become more globally inclusive, and it even has the potential to help preserve and revive traditions that are disappearing, especially by making it easier to digitize cultural knowledge. In my country, for example, we had a massive flood that damaged tons of historical documents that were kept in a university building. There were also fires in museums that erased invaluable parts of our history. If those materials had been digitized, they could have been preserved. So in that sense, AI and digitalization can be really powerful tools for protecting knowledge, but this also raises questions about ownership and control over that data.

For instance, I’ve learned a lot about traditional medicine from my mom and grandma. I find it fascinating to explore similar traditions, like traditional Chinese medicine, but it also makes me think that if this kind of knowledge is used to train AI models so someone from the other side of the world can access it, who really benefits from it as many of the teas and foods mentioned might not even exist in other regions. Would large pharmaceutical companies just take it and commercialize it? Maybe some knowledge is better protected and kept within the community rather than turned into data.

Another big concern is how dominant perspectives keep reinforcing themselves. As generative AI becomes more common, the same biased patterns get repeated, creating a feedback loop where one version of reality becomes the only story that survives. Even within the same country or culture, it’s often the same groups that get to decide what that culture is supposed to look like. So inclusivity isn’t just about representing everyone equally, it’s about equity, about questioning who gets to speak and whose stories are seen as worth telling.

For me, making AI truly inclusive isn’t just a technical problem, it’s a political and ethical one. It means rethinking how we evaluate these systems, moving away from Western-centered standards, and actually listening to the communities affected. But before we even talk about making AI inclusive, we need to tackle how the data itself already reflects deep inequalities, and I don’t think that’s something that will change easily or anytime soon.

We, as humans, don’t even fully understand our own cultures and histories to make fair judgments about what should be included or left out. So much knowledge has been lost through wars, poor preservation, colonization, or simply because it is so well hidden that we haven’t discovered it yet. Archaeologists and historians are still piecing together fragments of our past, trying to make sense of what remains.

And now, in the present, our data is controlled by a handful of tech corporations. It makes me think, sometimes even lose some sleep, about what we are losing today. Which voices will be preserved to tell our story in the future, and which ones will disappear, once again, because they didn’t fit the narrative or weren’t profitable enough to save?

### Mitigating bias

I think the first thing that needs to be addressed is data collection. It doesn’t matter how many filters or fancy features a model has, if the data is already biased, the output will be too. Evaluation also needs to move away from Western-centered benchmarks and become more community-centered. That might mean actually hiring people from the communities being represented to collect and interpret the data. I can build an economic model, but I have no idea how to interpret the results in a meaningful way. For this kind of qualitative data models, we already have people who understand it well enough to make informed decisions, so why wouldn’t there be someone to interpret results for a community the developer isn’t part of? Often it seems like if it’s not profitable, it’s not considered important, so why spend money on hiring someone else? The person that is creating these models might not have the lived experiences or know the struggles people from that community go through, so how could these developers possibly evaluate if the model has a fair representation? There’s already so much erasure of certain voices, we should at least try to avoid reinforcing stereotypes.

Developers also need to address power imbalances in representation. That means balancing how outsiders perceive a culture with what is actually true. Too often, cultures are presented as exotic or strange, when in reality they’re just as normal and complex as any other. For example, when people in the U.S. started using RedNote because TikTok was down, they noticed perspectives they hadn’t been exposed to before. Exploring both the internal and external views of a culture is important, it makes the results more accurate, nuanced, and meaningful. Developers need to pay attention to whose cultural narratives are being amplified and whose knowledge is being simplified or erased in the process.

I don’t think this is an easy task, and based on the data we have today, it might even be impossible. But in the end, this isn’t just about technical fixes, it is about the final use of the model that you are creating. AI should be treated as a cultural technology that carries real historical and social weight. By focusing on community-led evaluation, better data practices, and awareness of power dynamics, developers can start creating models that are not only more accurate but also more ethical and reflective of the diversity of real global communities. Honestly, it sounds a bit dystopian, but maybe it’s still good to hope.

### Taking into account the ever changing nature of culture and representation

Encoding a changing sense of representation seems really hard, and honestly, I’m not sure how it could be done in a way that actually reflects what a community is living and experiencing, while also respecting privacy and boundaries. One way to start might be to focus on content that comes from the community itself instead of only relying on static data that’s outdated or from an outside perspective that does not capture what the community is like.

Developers should also think about power structures and try to push back against them. Models shouldn’t just produce one-dimensional representations that flatten everything into stereotypes. Feedback loops could help too, so as communities and their cultures change, the model can change with them.

The big problem is that models are trained on static datasets, and most of the data we have today is biased or mostly reflects historically dominant perspectives. Even with better data and community input, there’s always a risk that complex identities get simplified or turned into stereotypes.

At the end of the day, I don’t think the goal should be perfect replication of cultural dynamism. It’s more about building systems that respect diversity, listen to communities, and can adapt when communities give feedback.

### History of technology and social erasure

Everywhere in history there have been strong power dynamics. From colonization to Hollywood’s representation of non-Western communities, it’s inevitable that these biases get transferred into AI models. By studying how past technologies, like photography, which was mostly only available to the rich, decided whose voices and experiences were shown, developers can better understand how AI might reproduce the same stereotypes.

History also shows that most cultures are only represented through an outside lens. Community-centered approaches could help fix this. However, the bigger issue is that so many voices were erased or never recorded because they were not considered important, and as the data is gone, fully correcting these biases is nearly impossible.

Actually understanding how erasing voices and perspectives is harmful in the long run is crucial, because we no longer have that data to rely on and have the full picture of human history. Focusing on equity rather than equality would also make AI models more fair, giving that most of the data we have today comes from those at the top of social hierarchies. We can learn a lot from the history of technology, but actually making these changes isn’t as simple as just recognizing patterns of erasure and discrimination.

Discussion question
--- 
### How can we ensure that the data used to train AI reflects diverse perspectives? Who gets to decide which voices are preserved to tell our history in the future, and can we trust those in power to make these decisions fairly?

**Why I included it** 
There are already so many voices being erased, and it’s not just because data isn’t preserved, it’s because this data, these opinions, can’t even be produced. People’s views, identities, and experiences are often hidden out of fear of consequences from oppressive systems. So the question isn’t only about AI models being fair or data being collected responsibly, it’s about a whole system that doesn’t allow certain data to exist in the first place. People say there were no LGBT individuals in the past, or no people with mental disabilities, but is that true, or were they just never able to produce that data so it could be recorded? Maybe it’s not the future erasing people who are different, but the present that prevents us from existing as the system doesn’t want us to.

Reflection
---
Reading the case study and thinking through the discussion questions gave me a better perspective on how important data is in shaping how people and cultures are represented, but it also made me understand that it’s not just about that. So much data has already been lost, and in many cases, it was never even allowed to be recorded because of hierarchical systems and social control. That means we’ll never have a full picture of history. Realizing this about the past is quite concerning because it shows the same thing could happen in the future, our stories and identities could be erased just like those before us. I do think AI models have the potential to help preserve communities and cultures, but there’s also a real risk depending on who controls these models, where the data goes, and how it’s used. It’s truly a gray area, nothing is purely good or purely bad.
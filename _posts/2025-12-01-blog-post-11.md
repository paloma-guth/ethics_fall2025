---
title: 'PRIV Blog 1: Complete Delete'
date: 2025-12-01
permalink: /posts/2025/12/blog-post-11/
tags:
  - privacy
  - case study
  - data
  - MIT Schwarzman
---

**When “delete” doesn’t mean gone**<br>
How systems of data preservation turn user intent into an illusion of control, keeping our digital past alive for institutional benefit.


**Case Study reading:**  
[Complete Delete: In Practice, Clicking 'Delete' Rarely Deletes. Should it?](https://mit-serc.pubpub.org/pub/fesqymtr/release/3?readingCollection=ca73f7c0)


Introduction
---
This case study explores the gap between what users believe happens when they click “delete” and what actually occurs in digital systems. It questions whether deletion should mean permanent erasure and examines the technical, ethical, and policy implications of data retention.

What struck me most, though, isn’t the technical side, it’s how the illusion of control shapes our relationship with technology. For me deletion isn’t just about freeing storage or cleaning a digital footprint, it’s about autonomy, dignity, and the right to move on and be forgotten. That so much of what we try to erase that still exists in hidden logs, backups, and corporate servers shows less about necessity and more about who holds power in the digital age. This isn’t only a question of how deletion works, it’s about who decides what’s truly gone, and why that choice so rarely belongs to us.

Discussion
---
### Approaches for informing users of the existence of remnant data on their systems.

One important approach is to be upfront about what “delete” actually means in a website or app. For example, when you delete your account on a social media platform, they usually mention that they’ll keep your data for a certain amount of time, but they don’t really explain what that looks like. Many users assume deletion erases data completely, but systems often follow Policy 1 (indeterminant deletion), meaning the data is removed from view but still exists somewhere. Users should be told directly when deletion does not actually erase your data and that the deleted information can be recovered using the right tools. Personally, whenever I delete an account on a website, I always get a vague warning that they’ll need to keep my data for security reasons, but they never explain what those reasons are. Another helpful approach is showing users which data still exists somewhere in the system. Some apps do this already, for example, most phone’s gallery shows recently deleted files before permanently removing them. Systems should also make clear when deleted data is still available in other services or backups, for example, if a photo remains synced to Google Drive or a similar service even after being deleted locally. Making these processes transparent would help reduce misunderstandings between what users think deletion does and what is actually happening in the background.

### Deleting information and information permanence.

Deletion feels like a basic right, a tool for managing your own identity, which is increasingly shaped by our online presence whether we like it or not. You should be able to remove personal, outdated, or simply embarrassing data. It’s practical too since it frees up storage. But that sense of control often feels like an illusion. I should know; I used to post everything, thinking little of the digital footprint I was leaving behind. There’s an unease in realizing that what you try to scrub away, like photos, rants, moments of vulnerability shared, might never truly be gone. That data can linger, accessible in ways we never intended. Deletion, then, isn’t just about storage, it’s an ethical necessity. People, especially young people, shouldn’t be hold accountable forever by minor mistakes.

Permanence, on the other hand, is what the system is built for. Its benefits are systemic, not personal. For institutions, it means evidence that can’t be destroyed, helping investigations. Authorities can [break through encryption on platforms](https://www.flashbackdata.com/5-cases-solved-forensic-data/#:~:text=Text%20messages%20and%20emails%20recovered,first%2Ddegree%20murder%20in%202013.) like WhatsApp, recover chats on Telegram, what’s meant to vanish instead becomes evidence. For technology itself, constant deletion and overwriting is inefficient; letting data sit is cheaper and improves performance. Permanence also enables backups, archives, and seamless operation.

In the end this is a conflict of values. Deletion serves individual dignity and privacy in a world that violates both. Permanence serves institutional memory, operational efficiency, and legal accountability. The problem is that the balance between these two is rarely decided by the individual, if they are even taken into account. It's decided by terms of service, by data center architecture, and by the laws that protect corporations more often than they protect people.

### Security issues behind governments having access to encrypted contents on devices

The position of these U.S. policymakers isn't just technically naive; it’s a fantasy of imperial control. They want a master key that only they can hold, in a world where locks and keys are fundamentally mathematical. The argument collapses under its own hypocrisy: why should the U.S. government have a privilege you’d rightly fear in the hands of China, Russia, or Iran? And whose devices are we talking about? Their own citizens'? The world's? The arrogance is astonishing for a nation that calls itself “the land of the free.”

I understand the appeal. Crimes happen in encrypted spaces, like child exploitation, terrorist plotting and so much more. The desire for law enforcement to have a tool to reach into that darkness is understandable and sometimes honorable. But you cannot build a backdoor that only the "good guys" can walk through. Any capability engineered for the U.S. government becomes a blueprint for every other government. You don’t get to weaken encryption just for your team, you weaken it for everyone, everywhere, forever. And the result isn’t a safer America, it’s a digital ecosystem riddled with fatal flaws, exploitable by hostile states, criminals, and abusive officials alike,which is something no country is lacking.

And let’s be clear about who those "good guys" are. This is the same U.S. law enforcement with well-documented racial bias, the same border agents who demand immigrants unlock their phones and justify their social media lives. Giving that system a master key isn’t about stopping rare horrors; it’s about normalizing access. A special access policy simply codifies and expands this overreach into a permanent architecture of surveillance.

This is the old colonial logic, just digitized. The core extracts what it wants, like data, access, control, from the periphery, which in this case is our own private lives. They call it security, but it’s just control. They say it’s for safety, but it creates a permanent vulnerability.

We already fail to stop terrible crimes with the access we have. Sacrificing everyone’s security for a little more hypothetical access isn’t a trade-off. It’s a surrender. It makes us all less safe to make the state feel more powerful. And history shows what happens when you give any state that kind of key, they don’t just use it on the criminals. They use it on everyone who looks like a threat, and they get to decide what that means.

### Redaction fails and file-deletion failures

At the core, both redaction failures and file-deletion failures are basically a betrayal of user intention. They exploit the same knowledge gap: what you think you’re doing, versus what the system actually does. In both cases, the interface gives you the illusion of control, such as a delete button or a black highlighter, while the underlying system keeps intact exactly what you’re trying to destroy. It’s just an elaborate theater. You draw a black box in Word thinking you’ve hidden a sentence, but the text is still there, just layered under a shape. You empty the Trash thinking the file’s gone, and yet it lingers until it’s overwritten. The problem isn’t just technical, it’s bluntly dishonesty. 

The consequences of both failures are different. A failed redaction is immediate. It exposes specific, sensitive detail, such as a name, a location, a number, inside a document meant to be seen. File deletion failure is broader. It leaves entire files, whole collections, just sitting there, waiting to be recovered. The tools to break this illusionary security are also different. Uncovering a bad redaction can be as simple as highlighting and copying the text. Recovering a deleted file might take forensic software, a bit more difficult but not impossible for someone with the motive.

The real difference comes down to whether a true fix even exists. Like mentioned in the article, systems like APFS can use cryptographic erasure, which destroys the key, and the file is truly gone. Proper redaction tools, if you use them right, strip the bytes from the document. The issue isn’t that secure destruction is impossible, it’s that the default, easy options, cheaper, are dangerously misleading. Microsoft designed the highlighter for emphasis, not for making things disappear. The delete button was built for convenience, not for permanence.

In the end, both redaction and deletion leave the user exposed, protecting the system’s priorities instead of the person’s intent. These systems value archives over individuals, preservation over privacy, convenience over control. I didn’t see the connection before as I always thought keeping deleted files would cost more resources, so I couldn’t understand why companies would benefit, but now it’s clear, institutions, corporations, states all gain a permanent record, a trail, a way to pull the past back into the present. It’s the same logic that keeps e-waste flowing to the Global South: out of sight out of mind but never truly gone. Until we demand tools that actually obey us, the users, not the companies, we’ll keep drawing black boxes over words that never fade and clicking delete on files that refuse to die.

New Discussion Question
---
If we recognize a “right to be forgotten” as a matter of digital dignity, as it should be and it is already in many countries, how can that right actually be enforced in systems designed to remember everything and who should be responsible for making true deletion possible?

*Why I included this question*<br>
For me, deleting your data should be a basic right, not just a product feature. I don’t trust tech companies to do it on their own if it doesn’t benefit them, which is why stronger policies and enforcement are necessary. Systems are built to retain information, and users alone can’t guarantee true deletion. Making this a right means holding platforms accountable, ensuring data is actually removed from all storage, and reclaiming some control over our digital presence, because it’s not just about privacy, it’s about dignity and autonomy.

Reflection
---
This case study made me think more about what happens in the background when you think you are deleting your data. I used to think lingering data was just a technical glitch or backup issue. Now it feels intentional. I’ve felt that unease before when closing an account or emptying my phone’s trash while knowing those files probably still exist somewhere. 

What bothers me most isn’t just that deletion is an illusion, it’s that once we believe something is gone, we stop thinking about it. Meanwhile, our data lives on in servers we’ll never access, analyzed and monetized without our consent. We click “delete” thinking we are taking control over our own data, but mostly we disappear from view, not from the system.

The discussion questions made me realize deletion shouldn’t be treated like a courtesy, it should be a right. If companies profit from our digital data and footprint, then accountability can’t be optional. This isn’t just about privacy, it’s about ownership, memory, and agency. Until our tools respect intention rather than extraction, we’ll remain as ghosts in someone else’s archive, unseen but present and profitable.

